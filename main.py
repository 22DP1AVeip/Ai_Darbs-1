import os
import time
import json
import requests
from dotenv import load_dotenv

HF_API_BASE = "https://router.huggingface.co/hf-inference"
DEFAULT_MODEL = "meta-llama/Llama-3.1-8B-Instruct"

def load_token():
    load_dotenv()
    token = os.getenv("HF_TOKEN") or os.getenv("HUGGINGFACE_API_KEY")
    if not token:
        raise ValueError("HF_TOKEN vai HUGGINGFACE_API_KEY nav atrasts .env failÄ!")
    return token

def query_model(model, prompt, token):
    url = f"{HF_API_BASE}/{model}"
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json"
    }
    payload = {"inputs": prompt}

    for attempt in range(3):
        resp = requests.post(url, headers=headers, json=payload, timeout=60)
        if resp.status_code == 200:
            break
        elif resp.status_code == 503:
            print("Modelis tiek ielÄdÄ“ts... gaidÄm 5 sekundes.")
            time.sleep(5)
            continue
        else:
            return f"KÄ¼Å«da: {resp.status_code} - {resp.text}"

    if resp.status_code != 200:
        return f"KÄ¼Å«da: {resp.status_code} - {resp.text}"

    try:
        data = resp.json()
    except Exception:
        return resp.text

    if isinstance(data, list) and len(data) > 0 and "generated_text" in data[0]:
        return data[0]["generated_text"].strip()
    elif isinstance(data, dict):
        if "generated_text" in data:
            return data["generated_text"].strip()
        elif "error" in data:
            return f"KÄ¼Å«da: {data['error']}"
        elif "choices" in data:
            c = data["choices"][0]
            if "message" in c and "content" in c["message"]:
                return c["message"]["content"].strip()

    return str(data)

def summarize_text(text, token):
    prompt = f"Kopsavilkums par Å¡o tekstu Ä«si un skaidri latviski:\n{text}"
    return query_model(DEFAULT_MODEL, prompt, token)

def generate_keywords(text, n, token):
    prompt = f"Izraksti {n} atslÄ“gvÄrdus no Å¡Ä« teksta (atdalÄ«tus ar komatiem):\n{text}"
    return query_model(DEFAULT_MODEL, prompt, token)

def generate_quiz(text, token):
    prompt = (
        f"Izveido 3 testa jautÄjumus ar 4 atbilÅ¾u variantiem (a, b, c, d), "
        f"balstoties uz Å¡o tekstu, un norÄdi pareizÄs atbildes:\n{text}"
    )
    return query_model(DEFAULT_MODEL, prompt, token)

if __name__ == "__main__":
    token = load_token()
    file = input("Ievadi .txt faila nosaukumu: ")
    if not file.endswith(".txt"):
        file += ".txt"

    if not os.path.exists(file):
        raise FileNotFoundError(f"Fails '{file}' netika atrasts!")

    with open(file, "r", encoding="utf-8") as f:
        text = f.read().strip()

    print("\nğŸ”¹ Ä¢enerÄ“ju kopsavilkumu...")
    summary = summarize_text(text, token)
    print("\nKopsavilkums:\n", summary)

    num_kw = int(input("\nCik atslÄ“gvÄrdus Ä£enerÄ“t?: "))
    print("\nğŸ”¹ Ä¢enerÄ“ju atslÄ“gvÄrdus...")
    keywords = generate_keywords(summary, num_kw, token)
    print("\nAtslÄ“gvÄrdi:\n", keywords)

    print("\nğŸ”¹ Ä¢enerÄ“ju jautÄjumus...")
    quiz = generate_quiz(summary, token)
    print("\nÄ¢enerÄ“tie jautÄjumi:\n", quiz)
